{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4604,"status":"ok","timestamp":1627567171933,"user":{"displayName":"Amir Dodangeh","photoUrl":"","userId":"03691380656141451894"},"user_tz":-270},"id":"5eXJrReqmtmw","outputId":"83c00250-d0d1-4620-91d1-f036257271eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","2021-07-29 13:59:27.433687: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","2021-07-29 13:59:29.412080: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n","2021-07-29 13:59:29.424120: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","2021-07-29 13:59:29.424174: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (53859f23610e): /proc/driver/nvidia/version does not exist\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!python3 \"/content/drive/My Drive/Colab Notebooks/Image Processing/Project/CloudXNet-master/cxn_model.py\""]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1627567171935,"user":{"displayName":"Amir Dodangeh","photoUrl":"","userId":"03691380656141451894"},"user_tz":-270},"id":"nj7alddVJsN3"},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Dropout, Dense, Flatten\n","from keras.optimizers import SGD\n","from keras.layers.convolutional import Conv2D, MaxPooling2D\n","from keras.utils import np_utils as u\n","import tensorflow as tf\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1jvKkVZmN99rYg7VSGB3GO9cinuxv7P33"},"id":"SrZNv6abv4gC","outputId":"f8475c23-d365-4a61-bd5c-d891a91bd28d"},"outputs":[],"source":["from keras.models import Model\n","from keras.layers import *\n","from keras.utils import *\n","import keras\n","from tensorflow.keras.optimizers import *\n","from keras.utils import np_utils as u\n","from keras import backend as K\n","from keras import *\n","import matplotlib.pyplot as plt\n","\n","\n","smooth = 0.0000001\n","\n","def aspp(x,out_shape):\n","  b0=SeparableConv2D(256,(1,1),padding=\"same\",use_bias=False)(x)\n","  b0=BatchNormalization()(b0)\n","  b0=Activation(\"relu\")(b0)\n","\n","  #b5=DepthwiseConv2D((3,3),dilation_rate=(3,3),padding=\"same\",use_bias=False)(x)\n","  #b5=BatchNormalization()(b5)\n","  #b5=Activation(\"relu\")(b5)\n","  #b5=SeparableConv2D(256,(1,1),padding=\"same\",use_bias=False)(b5)\n","  #b5=BatchNormalization()(b5)\n","  #b5=Activation(\"relu\")(b5)\n","  \n","  b1=DepthwiseConv2D((3,3),dilation_rate=(1,1),padding=\"same\",use_bias=False)(x)\n","  b1=BatchNormalization()(b1)\n","  b1=Activation(\"relu\")(b1)\n","  b1=SeparableConv2D(256,(1,1),padding=\"same\",use_bias=False)(b1)\n","  b1=BatchNormalization()(b1)\n","  b1=Activation(\"relu\")(b1)\n","  \n","  b2=DepthwiseConv2D((3,3),dilation_rate=(3,3),padding=\"same\",use_bias=False)(x)\n","  b2=BatchNormalization()(b2)\n","  b2=Activation(\"relu\")(b2)\n","  b2=SeparableConv2D(256,(1,1),padding=\"same\",use_bias=False)(b2)\n","  b2=BatchNormalization()(b2)\n","  b2=Activation(\"relu\")(b2)\t\n","  \n","  b3=DepthwiseConv2D((3,3),dilation_rate=(6,6),padding=\"same\",use_bias=False)(x)\n","  b3=BatchNormalization()(b3)\n","  b3=Activation(\"relu\")(b3)\n","  b3=SeparableConv2D(256,(1,1),padding=\"same\",use_bias=False)(b3)\n","  b3=BatchNormalization()(b3)\n","  b3=Activation(\"relu\")(b3)\n","  \n","  b4=AveragePooling2D(pool_size=(out_shape,out_shape))(x)\n","  b4=SeparableConv2D(256,(1,1),padding=\"same\",use_bias=False)(b4)\n","  b4=BatchNormalization()(b4)\n","  b4=Activation(\"relu\")(b4)\n","  #b4=UpSampling2D((out_shape,out_shape), interpolation='bilinear')(b4)\n","  #x=Concatenate()([b4,b0,b1,b2,b3])\n","  return x\n","\n","######\n","'''\n","def aspp2(x,out_shape):\n","  b0=SeparableConv2D(256,(1,1),padding=\"same\",use_bias=False)(x)\n","  b0=BatchNormalization()(b0)\n","  b0=Activation(\"relu\")(b0)\n","\n","  #b5=DepthwiseConv2D((3,3),dilation_rate=(3,3),padding=\"same\",use_bias=False)(x)\n","  #b5=BatchNormalization()(b5)\n","  #b5=Activation(\"relu\")(b5)\n","  #b5=SeparableConv2D(256,(1,1),padding=\"same\",use_bias=False)(b5)\n","  #b5=BatchNormalization()(b5)\n","  #b5=Activation(\"relu\")(b5)\n","  \n","  b1=DepthwiseConv2D((3,3),dilation_rate=(18,18),padding=\"same\",use_bias=False)(x)\n","  b1=BatchNormalization()(b1)\n","  b1=Activation(\"relu\")(b1)\n","  b1=SeparableConv2D(256,(1,1),padding=\"same\",use_bias=False)(b1)\n","  b1=BatchNormalization()(b1)\n","  b1=Activation(\"relu\")(b1)\n","  \n","  b2=DepthwiseConv2D((3,3),dilation_rate=(12,12),padding=\"same\",use_bias=False)(x)\n","  b2=BatchNormalization()(b2)\n","  b2=Activation(\"relu\")(b2)\n","  b2=SeparableConv2D(256,(1,1),padding=\"same\",use_bias=False)(b2)\n","  b2=BatchNormalization()(b2)\n","  b2=Activation(\"relu\")(b2)\t\n","  \n","  b3=DepthwiseConv2D((3,3),dilation_rate=(6,6),padding=\"same\",use_bias=False)(x)\n","  b3=BatchNormalization()(b3)\n","  b3=Activation(\"relu\")(b3)\n","  b3=SeparableConv2D(256,(1,1),padding=\"same\",use_bias=False)(b3)\n","  b3=BatchNormalization()(b3)\n","  b3=Activation(\"relu\")(b3)\n","  \n","  b4=AveragePooling2D(pool_size=(out_shape,out_shape))(x)\n","  b4=SeparableConv2D(256,(1,1),padding=\"same\",use_bias=False)(b4)\n","  b4=BatchNormalization()(b4)\n","  b4=Activation(\"relu\")(b4)\n","  #b4=UpSampling2D((out_shape,out_shape), interpolation='bilinear')(b4)\n","  #x=Concatenate()([b4,b0,b1,b2,b3])\n","  return x\n","'''\n","#########\n","def jacc_coef(y_true, y_pred):\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    return 1 - ((intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth))\n","\n","def bn_relu(input_tensor):\n","    \"\"\"It adds a Batch_normalization layer before a Relu\n","    \"\"\"\n","    input_tensor = BatchNormalization(axis=3)(input_tensor)\n","    return Activation(\"relu\")(input_tensor)\n","\n","\n","def contr_arm(input_tensor, filters, kernel_size):\n","    \"\"\"It adds a feedforward signal to the output of two following conv layers in contracting path\n","       TO DO: remove keras.layers.add and replace it with add only\n","    \"\"\"\n","\n","    x = SeparableConv2D(filters, kernel_size, padding='same')(input_tensor)\n","    x = bn_relu(x)\n","\n","    x = SeparableConv2D(filters, kernel_size, padding='same')(x)\n","    x = bn_relu(x)\n","\n","    filters_b = filters // 2\n","    kernel_size_b = (kernel_size[0]-2, kernel_size[0]-2)  # creates a kernl size of (1,1) out of (3,3)\n","\n","    x1 = SeparableConv2D(filters_b, kernel_size_b, padding='same')(input_tensor)\n","    x1 = bn_relu(x1)\n","\n","    x1 = concatenate([input_tensor, x1], axis=3)\n","    x = keras.layers.add([x, x1])\n","    x = Activation(\"relu\")(x)\n","    return x\n","\n","\n","def imprv_contr_arm(input_tensor, filters, kernel_size ):\n","    \"\"\"It adds a feedforward signal to the output of two following conv layers in contracting path\n","    \"\"\"\n","\n","    x = SeparableConv2D(filters, kernel_size, padding='same')(input_tensor)\n","    x = bn_relu(x)\n","\n","    x0 = SeparableConv2D(filters, kernel_size, padding='same')(x)\n","    x0 = bn_relu(x0)\n","\n","    x = SeparableConv2D(filters, kernel_size, padding='same')(x0)\n","    x = bn_relu(x)\n","\n","    filters_b = filters // 2\n","    kernel_size_b = (kernel_size[0]-2, kernel_size[0]-2)  # creates a kernl size of (1,1) out of (3,3)\n","\n","    x1 = SeparableConv2D(filters_b, kernel_size_b, padding='same')(input_tensor)\n","    x1 = bn_relu(x1)\n","\n","    x1 = concatenate([input_tensor, x1], axis=3)\n","\n","    x2 = SeparableConv2D(filters, kernel_size_b, padding='same')(x0)\n","    x2 = bn_relu(x2)\n","\n","    x = keras.layers.add([x, x1, x2])\n","    x = Activation(\"relu\")(x)\n","    return x\n","\n","\n","def bridge(input_tensor, filters, kernel_size):\n","    \"\"\"It is exactly like the identity_block plus a dropout layer. This block only uses in the valley of the UNet\n","    \"\"\"\n","\n","    x = SeparableConv2D(filters, kernel_size, padding='same')(input_tensor)\n","    x = bn_relu(x)\n","\n","    x = SeparableConv2D(filters, kernel_size, padding='same')(x)\n","    x = Dropout(.15)(x)\n","    x = bn_relu(x)\n","\n","    filters_b = filters // 2\n","    kernel_size_b = (kernel_size[0]-2, kernel_size[0]-2)  # creates a kernl size of (1,1) out of (3,3)\n","\n","    x1 =SeparableConv2D(filters_b, kernel_size_b, padding='same')(input_tensor)\n","    x1 = bn_relu(x1)\n","\n","    x1 = concatenate([input_tensor, x1], axis=3)\n","    x = keras.layers.add([x, x1])\n","    x = Activation(\"relu\")(x)\n","    return x\n","\n","\n","def conv_block_exp_path(input_tensor, filters, kernel_size):\n","    \"\"\"It Is only the convolution part inside each expanding path's block\n","    \"\"\"\n","\n","    x = Conv2D(filters, kernel_size, padding='same')(input_tensor)\n","    x = bn_relu(x)\n","\n","    x = Conv2D(filters, kernel_size, padding='same')(x)\n","    x = bn_relu(x)\n","    return x\n","\n","\n","def conv_block_exp_path3(input_tensor, filters, kernel_size):\n","    \"\"\"It Is only the convolution part inside each expanding path's block\n","    \"\"\"\n","\n","    x = Conv2D(filters, kernel_size, padding='same')(input_tensor)\n","    x = bn_relu(x)\n","\n","    x = Conv2D(filters, kernel_size, padding='same')(x)\n","    x = bn_relu(x)\n","\n","    x = Conv2D(filters, kernel_size, padding='same')(x)\n","    x = bn_relu(x)\n","    return x\n","\n","\n","def add_block_exp_path(input_tensor1, input_tensor2, input_tensor3):\n","    \"\"\"It is for adding two feed forwards to the output of the two following conv layers in expanding path\n","    \"\"\"\n","\n","    x = keras.layers.add([input_tensor1, input_tensor2, input_tensor3])\n","    x = Activation(\"relu\")(x)\n","    return x\n","\n","\n","def improve_ff_block4(input_tensor1, input_tensor2 ,input_tensor3, input_tensor4, pure_ff):\n","    \"\"\"It improves the skip connection by using previous layers feature maps\n","       TO DO: shrink all of ff blocks in one function/class\n","    \"\"\"\n","\n","    for ix in range(1):\n","        if ix == 0:\n","            x1 = input_tensor1\n","        x1 = concatenate([x1, input_tensor1], axis=3)\n","        x1 = MaxPooling2D(pool_size=(2, 2))(x1)\n","\n","    for ix in range(3):\n","        if ix == 0:\n","            x2 = input_tensor2\n","        x2 = concatenate([x2, input_tensor2], axis=3)\n","    x2 = MaxPooling2D(pool_size=(4, 4))(x2)\n","\n","    for ix in range(7):\n","        if ix == 0:\n","            x3 = input_tensor3\n","        x3 = concatenate([x3, input_tensor3], axis=3)\n","    x3 = MaxPooling2D(pool_size=(8, 8))(x3)\n","\n","    for ix in range(15):\n","        if ix == 0:\n","            x4 = input_tensor4\n","        x4 = concatenate([x4, input_tensor4], axis=3)\n","    x4 = MaxPooling2D(pool_size=(16, 16))(x4)\n","\n","    x = keras.layers.add([x1, x2, x3, x4, pure_ff])\n","    x = Activation(\"relu\")(x)\n","    return x\n","\n","\n","def improve_ff_block3(input_tensor1, input_tensor2, input_tensor3, pure_ff):\n","    \"\"\"It improves the skip connection by using previous layers feature maps\n","    \"\"\"\n","\n","    for ix in range(1):\n","        if ix == 0:\n","            x1 = input_tensor1\n","        x1 = concatenate([x1, input_tensor1], axis=3)\n","    x1 = MaxPooling2D(pool_size=(2, 2))(x1)\n","\n","    for ix in range(3):\n","        if ix == 0:\n","            x2 = input_tensor2\n","        x2 = concatenate([x2, input_tensor2], axis=3)\n","    x2 = MaxPooling2D(pool_size=(4, 4))(x2)\n","\n","    for ix in range(7):\n","        if ix == 0:\n","            x3 = input_tensor3\n","        x3 = concatenate([x3, input_tensor3], axis=3)\n","    x3 = MaxPooling2D(pool_size=(8, 8))(x3)\n","\n","    x = keras.layers.add([x1, x2, x3, pure_ff])\n","    x = Activation(\"relu\")(x)\n","    return x\n","\n","\n","def improve_ff_block2(input_tensor1, input_tensor2, pure_ff):\n","    \"\"\"It improves the skip connection by using previous layers feature maps\n","    \"\"\"\n","\n","    for ix in range(1):\n","        if ix == 0:\n","            x1 = input_tensor1\n","        x1 = concatenate([x1, input_tensor1], axis=3)\n","    x1 = MaxPooling2D(pool_size=(2, 2))(x1)\n","\n","    for ix in range(3):\n","        if ix == 0:\n","            x2 = input_tensor2\n","        x2 = concatenate([x2, input_tensor2], axis=3)\n","    x2 = MaxPooling2D(pool_size=(4, 4))(x2)\n","\n","    x = keras.layers.add([x1, x2, pure_ff])\n","    x = Activation(\"relu\")(x)\n","    return x\n","\n","\n","def improve_ff_block1(input_tensor1, pure_ff):\n","    \"\"\"It improves the skip connection by using previous layers feature maps\n","    \"\"\"\n","\n","    for ix in range(1):\n","        if ix == 0:\n","            x1 = input_tensor1\n","        x1 = concatenate([x1, input_tensor1], axis=3)\n","    x1 = MaxPooling2D(pool_size=(2, 2))(x1)\n","\n","    x = keras.layers.add([x1, pure_ff])\n","    x = Activation(\"relu\")(x)\n","    return x\n","\n","\n","def model_arch(input_rows=256, input_cols=256, num_of_channels=3, num_of_classes=1):\n","    inputs = Input((input_rows, input_cols, num_of_channels))\n","    conv1 = Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n","\n","    conv1 = contr_arm(conv1, 32, (3, 3))\n","    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","\n","    conv2 = contr_arm(pool1, 64, (3, 3))\n","    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","\n","    conv3 = contr_arm(pool2, 128, (3, 3))\n","    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","\n","    conv4 = contr_arm(pool3, 256, (3, 3))\n","    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n","\n","    conv5 = imprv_contr_arm(pool4, 512, (3, 3))\n","    pool5 = MaxPooling2D(pool_size=(2, 2))(conv5)\n","\n","    conv6 = bridge(pool5, 1024, (3, 3))\n","    \n","    conv6  = aspp(conv6,input_rows/32)\n","\n","    convT7 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(conv6)\n","    prevup7 = improve_ff_block4(input_tensor1=conv4, input_tensor2=conv3, input_tensor3=conv2, input_tensor4=conv1, pure_ff=conv5)\n","    up7 = concatenate([convT7, prevup7], axis=3)\n","    conv7 = conv_block_exp_path3(input_tensor=up7, filters=512, kernel_size=(3, 3))\n","    conv7 = add_block_exp_path(conv7, conv5, convT7)\n","\n","    convT8 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv7)\n","    prevup8 = improve_ff_block3(input_tensor1=conv3, input_tensor2=conv2, input_tensor3=conv1, pure_ff=conv4)\n","    up8 = concatenate([convT8, prevup8], axis=3)\n","    conv8 = conv_block_exp_path(input_tensor=up8, filters=256, kernel_size=(3, 3))\n","    conv8 = add_block_exp_path(input_tensor1=conv8, input_tensor2=conv4, input_tensor3=convT8)\n","\n","    convT9 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv8)\n","    prevup9 = improve_ff_block2(input_tensor1=conv2, input_tensor2=conv1, pure_ff=conv3)\n","    up9 = concatenate([convT9, prevup9], axis=3)\n","    conv9 = conv_block_exp_path(input_tensor=up9, filters=128, kernel_size=(3, 3))\n","    conv9 = add_block_exp_path(input_tensor1=conv9, input_tensor2=conv3, input_tensor3=convT9)\n","\n","    convT10 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv9)\n","    prevup10 = improve_ff_block1(input_tensor1=conv1, pure_ff=conv2)\n","    up10 = concatenate([convT10, prevup10], axis=3)\n","    conv10 = conv_block_exp_path(input_tensor=up10, filters=64, kernel_size=(3, 3))\n","    conv10 = add_block_exp_path(input_tensor1=conv10, input_tensor2=conv2, input_tensor3=convT10)\n","\n","    convT11 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv10)\n","    up11 = concatenate([convT11, conv1], axis=3)\n","    conv11 = conv_block_exp_path(input_tensor=up11, filters=32, kernel_size=(3, 3))\n","    conv11 = add_block_exp_path(input_tensor1=conv11, input_tensor2=conv1, input_tensor3=convT11)\n","\n","    conv12 = Conv2D(num_of_classes, (1, 1), activation='sigmoid')(conv11)\n","\n","    return Model(inputs=[inputs], outputs=[conv12])\n","\n","\n","\n","model = model_arch(input_rows=384, input_cols=384, num_of_channels=3, num_of_classes=1)\n","model.compile(optimizer = Adam(lr = 1e-4), loss = jacc_coef, metrics = [jacc_coef,'accuracy'])\n","len(model.layers)\n","\n","tf.keras.utils.plot_model(model, show_shapes=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"B0ZLtsKbqu9F"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n","100%|██████████| 350/350 [20:58\u003c00:00,  3.60s/it]\n","100%|██████████| 10/10 [00:36\u003c00:00,  3.65s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/15\n","166/166 [==============================] - 1133s 7s/step - loss: 0.6339 - jacc_coef: 0.6339 - accuracy: 0.7722 - val_loss: 0.9779 - val_jacc_coef: 0.9779 - val_accuracy: 0.6735\n","Epoch 2/15\n","166/166 [==============================] - 1175s 7s/step - loss: 0.3843 - jacc_coef: 0.3843 - accuracy: 0.9214 - val_loss: 0.9879 - val_jacc_coef: 0.9879 - val_accuracy: 0.6735\n","Epoch 3/15\n","166/166 [==============================] - 1186s 7s/step - loss: 0.2948 - jacc_coef: 0.2948 - accuracy: 0.9577 - val_loss: 0.9836 - val_jacc_coef: 0.9836 - val_accuracy: 0.6771\n","Epoch 4/15\n","166/166 [==============================] - 1182s 7s/step - loss: 0.2874 - jacc_coef: 0.2874 - accuracy: 0.9569 - val_loss: 0.1900 - val_jacc_coef: 0.1900 - val_accuracy: 0.9585\n","Epoch 5/15\n","166/166 [==============================] - 1186s 7s/step - loss: 0.2918 - jacc_coef: 0.2918 - accuracy: 0.9543 - val_loss: 0.2091 - val_jacc_coef: 0.2091 - val_accuracy: 0.9542\n","Epoch 6/15\n","166/166 [==============================] - 1169s 7s/step - loss: 0.2446 - jacc_coef: 0.2446 - accuracy: 0.9647 - val_loss: 0.1475 - val_jacc_coef: 0.1475 - val_accuracy: 0.9654\n","Epoch 7/15\n","166/166 [==============================] - 1176s 7s/step - loss: 0.2354 - jacc_coef: 0.2354 - accuracy: 0.9682 - val_loss: 0.1475 - val_jacc_coef: 0.1475 - val_accuracy: 0.9632\n","Epoch 8/15\n","166/166 [==============================] - 1190s 7s/step - loss: 0.2312 - jacc_coef: 0.2312 - accuracy: 0.9703 - val_loss: 0.1402 - val_jacc_coef: 0.1402 - val_accuracy: 0.9641\n","Epoch 9/15\n","166/166 [==============================] - 1183s 7s/step - loss: 0.2282 - jacc_coef: 0.2282 - accuracy: 0.9672 - val_loss: 0.1523 - val_jacc_coef: 0.1523 - val_accuracy: 0.9657\n","Epoch 10/15\n","166/166 [==============================] - 1059s 6s/step - loss: 0.2247 - jacc_coef: 0.2247 - accuracy: 0.9727 - val_loss: 0.1518 - val_jacc_coef: 0.1518 - val_accuracy: 0.9633\n","Epoch 11/15\n","166/166 [==============================] - 1137s 7s/step - loss: 0.2314 - jacc_coef: 0.2314 - accuracy: 0.9725 - val_loss: 0.1461 - val_jacc_coef: 0.1461 - val_accuracy: 0.9619\n","Epoch 12/15\n","166/166 [==============================] - 1180s 7s/step - loss: 0.1963 - jacc_coef: 0.1963 - accuracy: 0.9666 - val_loss: 0.1831 - val_jacc_coef: 0.1831 - val_accuracy: 0.9597\n","Epoch 13/15\n","135/166 [=======================\u003e......] - ETA: 3:36 - loss: 0.2041 - jacc_coef: 0.2041 - accuracy: 0.9683"]}],"source":["import os\n","import tensorflow as tf\n","from tensorflow import keras\n","import random\n","from keras.callbacks import TensorBoard\n","#from keras import backend as K\n","from skimage.io import imread, imshow\n","from skimage.transform import resize\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm \n","import numpy as np\n","from PIL import Image\n","import sys\n","sys.path.append(\"/content/drive/My Drive/Colab Notebooks/Image Processing/Project/CloudXNet-master\")\n","from cxn_model import *\n"," \n","IMG_WIDTH = 256\n","IMG_HEIGHT = 256\n","IMG_CHANNELS = 3\n"," \n","TRAIN_PATH_R = '/content/drive/My Drive/Colab Notebooks/Image Processing/Project/dataset/B4/train/'  #change path \n","TRAIN_PATH_G = '/content/drive/My Drive/Colab Notebooks/Image Processing/Project/dataset/B3/train/'\n","TRAIN_PATH_B = '/content/drive/My Drive/Colab Notebooks/Image Processing/Project/dataset/B2/train/'\n"," \n","TEST_PATH_R = '/content/drive/My Drive/Colab Notebooks/Image Processing/Project/dataset/B4/test/'\n","TEST_PATH_G = '/content/drive/My Drive/Colab Notebooks/Image Processing/Project/dataset/B3/test/'\n","TEST_PATH_B = '/content/drive/My Drive/Colab Notebooks/Image Processing/Project/dataset/B2/test/'\n"," \n","X_train = np.zeros((350, IMG_WIDTH, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n","Y_train = np.zeros((350, IMG_WIDTH, IMG_WIDTH, 1), dtype=np.float32)\n","img = np.zeros((IMG_WIDTH, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n"," \n","tr=np.zeros(350)\n","te=np.zeros(10)\n"," \n","for i in range(350):\n","       tr[i]=i;\n","for i in range(10):\n","       te[i]=i;\n"," \n"," \n","for n, id_ in tqdm(enumerate(tr),total=350):\n","       red    = Image.open(TRAIN_PATH_R + str(int(id_)) + '.png').convert('L')\n","       green  = Image.open(TRAIN_PATH_G + str(int(id_)) + '.png').convert('L')\n","       blue   = Image.open(TRAIN_PATH_B + str(int(id_)) + '.png').convert('L')\n","       \n","       rgb = Image.merge(\"RGB\",(red,green,blue))\n","       img_b = np.asarray(rgb) \n","       \n","       #img_r = imread(TRAIN_PATH_R + str(int(id_)) + '.png')[:,:,:IMG_CHANNELS]\n","       #img_g = imread(TRAIN_PATH_G + str(int(id_)) + '.png')[:,:,:IMG_CHANNELS]\n","       #img_b = imread(TRAIN_PATH_B + str(int(id_)) + '.png')[:,:,:IMG_CHANNELS]\n"," \n","       #img_r = resize(img_r, (IMG_HEIGHT, IMG_WIDTH, 1), mode='constant', preserve_range=True)\n","       #img_g = resize(img_g, (IMG_HEIGHT, IMG_WIDTH, 1), mode='constant', preserve_range=True)\n","       #img_b = resize(img_b, (IMG_HEIGHT, IMG_WIDTH, 1), mode='constant', preserve_range=True)\n"," \n","       img_b = resize(img_b, (IMG_HEIGHT, IMG_WIDTH, 3), mode='constant', preserve_range=True)\n","       img_b=img_b/255.0\n","       #for i in range(256):\n","       #       img[i] = np.concatenate((img_r[i],img_g[i],img_b[i]), axis=1)\n","       \n","       X_train[n] = img_b\n","       \n","       #mask = Image.open('/content/drive/My Drive/Colab Notebooks/dataset/BQA/train/' + str(int(102+id_)) + '.png').convert('L')\n","       mask = imread('/content/drive/My Drive/Colab Notebooks/Image Processing/Project/dataset/BQA/train/' + str(int(id_)) + '.png')[:,:,:IMG_CHANNELS]\n","       mask1 = resize(mask, (IMG_HEIGHT, IMG_WIDTH, 1), mode='constant', preserve_range=True)\n","       #mask1 = np.asarray(mask)\n","       #mask1 = resize(mask1, (IMG_HEIGHT, IMG_WIDTH, 1), mode='constant', preserve_range=True)\n"," \n","       Y_train[n] =mask1/255.0\n","       for io in range(256):\n","         for jo in range(256):\n","           if (Y_train[n][io][jo]\u003e0.3):\n","             Y_train[n][io][jo]=1\n","           else:\n","             Y_train[n][io][jo]=0\n","       \n"," \n","# for test images \n"," \n","X_test = np.zeros((10, IMG_WIDTH, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n","Y_test = np.zeros((10, IMG_WIDTH, IMG_WIDTH, 1), dtype=np.float32)\n","img = np.zeros((IMG_WIDTH, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n","sizes_test = []\n"," \n"," \n","for n, id_ in tqdm(enumerate(te),total=10):\n","       red    = Image.open(TEST_PATH_R + str(170+int(id_)) + '.png').convert('L')\n","       green  = Image.open(TEST_PATH_G + str(170+int(id_)) + '.png').convert('L')\n","       blue   = Image.open(TEST_PATH_B + str(170+int(id_)) + '.png').convert('L')\n","       \n","       rgb = Image.merge(\"RGB\",(red,green,blue))\n","       img_b = np.asarray(rgb)  \n"," \n","       img_b = resize(img_b, (IMG_HEIGHT, IMG_WIDTH, 3), mode='constant', preserve_range=True)\n","       img_b=img_b/255.0\n","       #for i in range(256):\n","       #       img[i] = np.concatenate((img_r[i],img_g[i],img_b[i]), axis=1)\n","       \n","       X_test[n] = img_b\n"," \n","       mask = imread('/content/drive/My Drive/Colab Notebooks/Image Processing/Project/dataset/BQA/test/' + str(int(170+id_)) + '.png')[:,:,:IMG_CHANNELS]\n","       mask1 = resize(mask, (IMG_HEIGHT, IMG_WIDTH, 1), mode='constant', preserve_range=True)\n","       #mask1 = np.asarray(mask)\n","       #mask1 = resize(mask1, (IMG_HEIGHT, IMG_WIDTH, 1), mode='constant', preserve_range=True)\n"," \n","       Y_test[n] =mask1/255.0\n","       for io in range(256):\n","         for jo in range(256):\n","           if (Y_test[n][io][jo]\u003e0.3):\n","             Y_test[n][io][jo]=1\n","           else:\n","             Y_test[n][io][jo]=0\n","\n","\n","model = model_arch(input_rows=256, input_cols=256, num_of_channels=3, num_of_classes=1)\n","model.compile(optimizer = Adam(lr = 1e-4), loss = jacc_coef, metrics = [jacc_coef,'accuracy'])\n","\n","\n","#checkpointer = tf.keras.callbacks.ModelCheckpoint('model_for_nuclei.h5' , verbose=1, save_best_only=True)\n","#callbacks = [tf.keras.callbacks.EarlyStopping(patience=50, monitor='val_loss'),tf.keras.callbacks.TensorBoard(log_dir=\"logs\")]\n","\n","\n","results = model.fit(X_train, Y_train, validation_split=0.05, batch_size=2, epochs=2, verbose=1)   #, callbacks=[cp_callback])\n","\n","\n","preds_train = model.predict(X_train, verbose=1)\n","preds_val = model.predict(X_train[int(X_train.shape[0]*0.95):], verbose=1)\n","preds_test = model.predict(X_test, verbose=1)\n","\n","\n","preds_train_t = (preds_train \u003e 0.5).astype(np.float32)\n","preds_val_t = (preds_val \u003e 0.5).astype(np.float32)\n","preds_test_t = (preds_test \u003e 0.5).astype(np.float32)\n","\n","train_acc = model.evaluate(X_train, Y_train, verbose=1)\n","test_acc = model.evaluate(X_test, Y_test, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJyhKjyHcPSj"},"outputs":[],"source":["  \n","from sklearn.metrics import confusion_matrix\n","\n","def precision(gt,mask):\n"," gt = gt.flatten()\n"," mask = mask.flatten()\n"," tn,fp,fn,tp = confusion_matrix(gt,mask).ravel()\n"," prec = tp/(tp+fp)\n"," return(prec)\n","\n","####recall---\n","def recall(gt,mask):\n"," gt = gt.flatten()\n"," mask = mask.flatten()\n"," tn,fp,fn,tp = confusion_matrix(gt,mask).ravel()\n"," rec = tp/(tp+fn)\n"," return(rec)\n","\n","###f1 score--\n","\n","def f1_score(prec,rec):\n"," f1 = 2*(prec*rec)/(prec+rec)\n"," return f1\n","\n","  ### jaccard \n","def jaccard(gt,mask):\n"," gt = gt.flatten()\n"," mask = mask.flatten()\n"," tn,fp,fn,tp = confusion_matrix(gt,mask).ravel()\n"," rec = tp/(tp+fn+fp)\n"," return(rec)\n","\n","  ### jaccard \n","def Overall(gt,mask):\n"," gt = gt.flatten()\n"," mask = mask.flatten()\n"," tn,fp,fn,tp = confusion_matrix(gt,mask).ravel()\n"," rec = (tp+tn)/(tp+fp+fn+tn)\n"," return(rec)\n","\n","\n","\n","\n","\n","###aji score\n","\n","def get_fast_aji(true, pred):\n","    \n","    true = np.copy(true) # ? do we need this\n","    pred = np.copy(pred)\n","    true_id_list = list(np.unique(true))\n","    pred_id_list = list(np.unique(pred))\n","\n","    true_masks = [None,]\n","    for t in true_id_list[1:]:\n","        t_mask = np.array(true == t, np.uint8)\n","        true_masks.append(t_mask)\n","    \n","    pred_masks = [None,]\n","    for p in pred_id_list[1:]:\n","        p_mask = np.array(pred == p, np.uint8)\n","        pred_masks.append(p_mask)\n","    \n","    # prefill with value\n","    pairwise_inter = np.zeros([len(true_id_list) -1, \n","                               len(pred_id_list) -1], dtype=np.float64)\n","    pairwise_union = np.zeros([len(true_id_list) -1, \n","                               len(pred_id_list) -1], dtype=np.float64)\n","\n","    # caching pairwise\n","    for true_id in true_id_list[1:]: # 0-th is background\n","        t_mask = true_masks[int(true_id)]\n","        pred_true_overlap = pred[t_mask \u003e 0]\n","        pred_true_overlap_id = np.unique(pred_true_overlap)\n","        pred_true_overlap_id = list(pred_true_overlap_id)\n","        for pred_id in pred_true_overlap_id:\n","            if pred_id == 0: # ignore\n","                continue # overlaping background\n","            p_mask = pred_masks[int(pred_id)]\n","            total = (t_mask + p_mask).sum()\n","            inter = (t_mask * p_mask).sum()\n","            pairwise_inter[int(true_id)-1, int(pred_id)-1] = inter\n","            pairwise_union[int(true_id)-1, int(pred_id)-1] = total - inter\n","    #\n","    pairwise_iou = pairwise_inter / (pairwise_union + 1.0e-6)\n","    # pair of pred that give highest iou for each true, dont care \n","    # about reusing pred instance multiple times\n","#paired_pred = np.argmax(pairwise_iou, axis=1)\n","#pairwise_iou = np.max(pairwise_iou, axis=1)\n","    # exlude those dont have intersection\n","#paired_true = np.nonzero(pairwise_iou \u003e 0.0)[0]\n","#paired_pred = paired_pred[paired_true]\n","    # print(paired_true.shape, paired_pred.shape)\n","#overall_inter = (pairwise_inter[paired_true, paired_pred]).sum()\n","#overall_union = (pairwise_union[paired_true, paired_pred]).sum()\n","    #\n","#paired_true = (list(paired_true + 1)) # index to instance ID\n","#paired_pred = (list(paired_pred + 1))\n","    # add all unpaired GT and Prediction into the union\n","#unpaired_true = np.array([idx for idx in true_id_list[1:] if idx not in paired_true])\n","#unpaired_pred = np.array([idx for idx in pred_id_list[1:] if idx not in paired_pred])\n","#for true_id in unpaired_true:\n","#overall_union += true_masks[true_id].sum()\n","#for pred_id in unpaired_pred:\n","#overall_union += pred_masks[pred_id].sum()\n","    #\n","#aji_score = overall_inter / overall_union\n","#return aji_score\n","\n","sum = 0\n","for i in range(len(Y_test)):\n"," sum = sum + precision(Y_test[i],preds_test_t[i])\n","prec = sum/len(Y_test)\n","\n","sum = 0\n","for i in range(len(Y_test)):\n"," sum = sum + recall(Y_test[i],preds_test_t[i])\n","rec = sum/len(Y_test)\n","\n","sum = 0\n","for i in range(len(Y_test)):\n"," sum = sum + jaccard(Y_test[i],preds_test_t[i])\n","jaccard1 = sum/len(Y_test)\n","\n","\n","sum = 0\n","for i in range(len(Y_test)):\n"," sum = sum + Overall(Y_test[i],preds_test_t[i])\n","Overall1 = sum/len(Y_test)\n","\n","\n","f1 = f1_score(prec,rec)\n","aji = get_fast_aji(Y_test,preds_test_t)\n","\n","print(\"Jaccard Index\", jaccard1)\n","print(\"final f1\", f1)\n","print(\"final precision\",prec)\n","print(\"final recall\",rec)\n","print(\"Overall Accuracy\",Overall1)\n","print(\"final aji\",aji)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGWaQ1WhMens"},"outputs":[],"source":["results.history['accuracy']\n","print(\"Accuracy is:\",Overall1)\n","\n","from matplotlib import pyplot\n","pyplot.plot(results.history['accuracy'])\n","pyplot.title(\"Accuracy\")\n","plt.ylabel(\"accuracy\")\n","plt.xlabel(\"epoch\")\n","pyplot.show()\n","#pyplot.savefig()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2pSETESg9uai"},"outputs":[],"source":["results.history['loss']\n","#print(\"loss is:\" , val_loss)\n","\n","pyplot.plot(results.history[\"loss\"])\n","pyplot.title(\"loss\")\n","plt.ylabel(\"loss\")\n","plt.xlabel(\"epoch\")\n","pyplot.show()\n","#pyplot.savefig()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I61edex4ZJYI"},"outputs":[],"source":["def display_learning_curves(history):\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n","\n","    ax1.plot(history.history[\"loss\"])\n","    ax1.plot(history.history[\"val_loss\"])\n","    ax1.legend([\"train\", \"test\"], loc=\"upper right\")\n","    ax1.set_xlabel(\"Epochs\")\n","    ax1.set_ylabel(\"Loss\")\n","\n","    ax2.plot(history.history[\"accuracy\"])\n","    #ax2.plot(history.history[\"val_acc\"])\n","    ax2.legend([\"train\", \"test\"], loc=\"upper right\")\n","    ax2.set_xlabel(\"Epochs\")\n","    ax2.set_ylabel(\"Accuracy\")\n","    plt.show()\n","    #plt.savefig()\n","\n","\n","\n","\n","display_learning_curves(results)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP01t73KMX6VU4tr4L5ALfE","name":"IP+PROJECT.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}